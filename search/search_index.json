{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"es-vocab Documentation Welcome to the es-vocab documentation. This documentation provides a comprehensive guide on what es-vocab is, why it is essential, and how to use it effectively. Overview es-vocab is a controlled vocabulary service designed to manage and serve standardized terms and metadata for various projects. It ensures consistency and interoperability across different systems and users by providing access to a centralized repository of terms and associated metadata. The service is structured around three main components: WGCM_CVs Repository : The main repository that inventories all possible values of terms along with their metadata. Project-Specific Repositories : Each project (e.g., CMIP6Plus) has its own repository where collections of IDs from the WGCM_CVs repository are listed, with the possibility to add or change metadata as necessary. Service Interfaces : es-vocab serves vocabulary to different users via a REST API, a website, and a SPARQL endpoint for web semantic interoperability. What is es-vocab? es-vocab is a service that manages controlled vocabularies, which are standardized sets of terms and their definitions. These vocabularies are crucial for ensuring that different systems and users refer to the same concepts in a consistent manner. Components WGCM_CVs Repository : Data-Descriptor Categories : Inventory of all possible term values and associated metadata. Project-Specific Repositories : Collections : Lists of IDs from the WGCM_CVs repository specific to each project, with potential additional or modified metadata. Service Interfaces : REST API : For programmatic access. details here Website : For human users. more details here SPARQL Endpoint : For web semantic interoperability, with IRIs pointing to the website. more details here Why use es-vocab? Controlled vocabularies are essential for several reasons: Consistency : Ensures that all users and systems use the same terms in the same way. Interoperability : Facilitates communication and data exchange between different systems and projects. Efficiency : Reduces the need for rework and clarifications by providing a single source of truth for terms and their definitions. Quality Control : Helps maintain high data quality by enforcing standardized term usage.","title":"Home"},{"location":"#es-vocab-documentation","text":"Welcome to the es-vocab documentation. This documentation provides a comprehensive guide on what es-vocab is, why it is essential, and how to use it effectively.","title":"es-vocab Documentation"},{"location":"#overview","text":"es-vocab is a controlled vocabulary service designed to manage and serve standardized terms and metadata for various projects. It ensures consistency and interoperability across different systems and users by providing access to a centralized repository of terms and associated metadata. The service is structured around three main components: WGCM_CVs Repository : The main repository that inventories all possible values of terms along with their metadata. Project-Specific Repositories : Each project (e.g., CMIP6Plus) has its own repository where collections of IDs from the WGCM_CVs repository are listed, with the possibility to add or change metadata as necessary. Service Interfaces : es-vocab serves vocabulary to different users via a REST API, a website, and a SPARQL endpoint for web semantic interoperability.","title":"Overview"},{"location":"#what-is-es-vocab","text":"es-vocab is a service that manages controlled vocabularies, which are standardized sets of terms and their definitions. These vocabularies are crucial for ensuring that different systems and users refer to the same concepts in a consistent manner.","title":"What is es-vocab?"},{"location":"#components","text":"WGCM_CVs Repository : Data-Descriptor Categories : Inventory of all possible term values and associated metadata. Project-Specific Repositories : Collections : Lists of IDs from the WGCM_CVs repository specific to each project, with potential additional or modified metadata. Service Interfaces : REST API : For programmatic access. details here Website : For human users. more details here SPARQL Endpoint : For web semantic interoperability, with IRIs pointing to the website. more details here","title":"Components"},{"location":"#why-use-es-vocab","text":"Controlled vocabularies are essential for several reasons: Consistency : Ensures that all users and systems use the same terms in the same way. Interoperability : Facilitates communication and data exchange between different systems and projects. Efficiency : Reduces the need for rework and clarifications by providing a single source of truth for terms and their definitions. Quality Control : Helps maintain high data quality by enforcing standardized term usage.","title":"Why use es-vocab?"},{"location":"DD_creation_example/","text":"Adding a new datadescriptor Lets take an example to illustrate the procedure : we will add the activity datadescriptor 1. Define the Linkml model the linkml model permits to automate differents things that are usefull to deal with the controled vocabulary: generate jsonschema model, generate pydantic model. Depends on what you prefer generate jsonld validation of terms get actual Universe repository git clone https://github.com/ESPRI-Mod/WGCM_CVs.git swith to a new branch git checkout -b newDD 1. Define the linkml model the linkml models are in the directory \"src/wgcm_cvs/schemas\". we can look at one similar datadescriptor in order to create the new one. There 3 distincts methods to define the CVs : list of valid terms a regex to valid a term a composite that can be a composition of the previous one. Copy existing similar data descriptor in our example activity . This a list of valid term. institution.yaml is also a list of term. this will be our starting point. Lets copy the latter as activity.yaml . Changes according to our new data descriptor id : change the id and the name accordindly the prefixes used by the web semantic part of the service is pretty defined. just change the institution in activity in the key and the value : activity: http://es-vocab.ipsl.fr/activity/ change the default prefix to freshly created : default_prefix: activity the model classes : First thing first let's name our class like the datadesciptor with a upper first letter. classes: Activity: lets add a description : what is an activty ? description :an 'activity' refers to a coordinated set of modeling experiments designed to address specific scientific questions or objectives. Each activity is focused on different aspects of climate science and utilizes various models to study a wide range of climate phenomena. Activities are often organized around key research themes and may involve multiple experiments, scenarios, and model configurations. class_uri is used to define the appropriate type in the context of web semantic : lets use our prefix and datadescriptor name : class_uri: esvocab:activity the main course: the attibutes. the basic one : change the slot_uri validation_method doesn't have to change since we have started from a descriptor that have the same validation method . here a list of terms here we have to define the model (i.e keys) for our datadescriptor : activity is defined with a standard acronym and a long name. We just have to keep name thats will be the same as cmip_acronym and create a long_name . In the new CMIP6Plus CV, there is also an url to document the activity. Lets add a url * attribute to our model attributes: id: slot_uri: activity:id range: string required: true identifier: true validation_method : required: true range : string ifabsent: string(list) slot_uri: es-vocab:validation_method name: range: string required: true long_name: range: string required: true cmip_acronym: range: string required: true url: range : string required: false the linkml model is defined. This will be used by the repository to validate the defined standard terms. 2. Define terms As a starting point for activity terms. we will use the known activities from CMIP6 and CMIP6Plus : for CMIP6 : \"activity_id\":{ \"AerChemMIP\":\"Aerosols and Chemistry Model Intercomparison Project\", \"C4MIP\":\"Coupled Climate Carbon Cycle Model Intercomparison Project\", \"CDRMIP\":\"Carbon Dioxide Removal Model Intercomparison Project\", \"CFMIP\":\"Cloud Feedback Model Intercomparison Project\", \"CMIP\":\"CMIP DECK: 1pctCO2, abrupt4xCO2, amip, esm-piControl, esm-historical, historical, and piControl experiments\", \"CORDEX\":\"Coordinated Regional Climate Downscaling Experiment\", \"DAMIP\":\"Detection and Attribution Model Intercomparison Project\", \"DCPP\":\"Decadal Climate Prediction Project\", \"DynVarMIP\":\"Dynamics and Variability Model Intercomparison Project\", \"FAFMIP\":\"Flux-Anomaly-Forced Model Intercomparison Project\", \"GMMIP\":\"Global Monsoons Model Intercomparison Project\", \"GeoMIP\":\"Geoengineering Model Intercomparison Project\", \"HighResMIP\":\"High-Resolution Model Intercomparison Project\", \"ISMIP6\":\"Ice Sheet Model Intercomparison Project for CMIP6\", \"LS3MIP\":\"Land Surface, Snow and Soil Moisture\", \"LUMIP\":\"Land-Use Model Intercomparison Project\", \"OMIP\":\"Ocean Model Intercomparison Project\", \"PAMIP\":\"Polar Amplification Model Intercomparison Project\", \"PMIP\":\"Palaeoclimate Modelling Intercomparison Project\", \"RFMIP\":\"Radiative Forcing Model Intercomparison Project\", \"SIMIP\":\"Sea Ice Model Intercomparison Project\", \"ScenarioMIP\":\"Scenario Model Intercomparison Project\", \"VIACSAB\":\"Vulnerability, Impacts, Adaptation and Climate Services Advisory Board\", \"VolMIP\":\"Volcanic Forcings Model Intercomparison Project\" }, for CMIP6Plus : \"CMIP\": { \"URL\": \"https://gmd.copernicus.org/articles/9/1937/2016/gmd-9-1937-2016.pdf\", \"long_name\": \"CMIP DECK: 1pctCO2, abrupt4xCO2, amip, esm-piControl, esm-historical, historical, and piControl experiments\" }, \"LESFMIP\": { \"URL\": \"https://www.frontiersin.org/articles/10.3389/fclim.2022.955414/full\", \"long_name\": \"The Large Ensemble Single Forcing Model Intercomparison Project\" } in the directory \"data_decriptor\" : create a activity/terms directories. Inside one term is define in one json file. We will use a python script to create those. a little script to do that is stored in \"script/DD_specific/create_activity.py\". Eventually terms a created : an example : pmip.json { \"id\": \"pmip\", \"cmip_acronym\": \"PMIP\", \"long_name\": \"Palaeoclimate Modelling Intercomparison Project\", \"url\": null } 3. Validate model and terms locally it is possible to validate model and terms locally tahnks to linkml library : for instance : pdm run linkml-validate --schema schemas/activity.yaml --target-class ../../data_descriptors/source/terms/pmip.json Or you can let github action do this : 4. Here Magic happen Push changes git commit -am \"create activity datadescriptor and terms\" git push --set-upstream origin activityDD validation then in github interface on WGCM_CVs repository CI/CD : Validation You have to wait for CI/CD to check that everything is good, if not the merge will be cancel ! what does the CI/CD : create pydantic and jsonscheme model in \"datadescriptors/activity/models\" create the jsonld file version of each terms in \"datadesciptors/activity/terms\" validate the terms with the models. validation CI/CD can be view in Action tab. for instance with this procedure : the validate failed because there was a type in the linkml file name ! the validate failed cause of pydantic model why ? here the problem need to be tested in local. we can run the model creation and validate script : pdm run python scripts/generate_models.py the models are stored next to terms, i cant see problem with the model. lets try to validate locally pdm run python scripts/validate.py it validates everythings through json schema but not pydantic : Field required [type=missing, input_value={'id': 'aerchemmip', 'cmi...n Project', 'url': None}, input_type=dict] there is a missing field, after investigation : the python script that create the terms does not include name attribute. Lets repair that ! pdm run python scripts/validate.py YES !, validation is OK ! lets push changes to the origin repository Create a pull request if validation step went well, create a pull request. This will be merged into main asap. The magic github repository is configured to send a message to the CV Service. It will be update automaticaly !!! Define projects collection Each project does not necesseraly use or even valid all the terms in the univers data descriptor. collections of valid id have to be define in each project : for instance, we will add activity collection id for CMIP6Plus_CVs : clone the repo git clone https://github.com/ESPRI-Mod/CMIP6Plus_CVs.git create a new branch git checkout -b activity create collection in directory collections : Lets start from the institution.json and copy it as activity.json { \"@context\":{ \"@base\": \"http://es-vocab.ipsl.fr/Institution\" }, \"@graph\":[ {\"@id\":\"ipsl\", \"@type\":\"institution\", \"name\" : \"Institut Pierre-Simon Laplace modified\", \"location\":{ \"city\":\"Paris5\" }, \"myprop\":\"42\" }, { \"@id\":\"llnl\", \"@type\":\"institution\"} ] } this one is a show case to view the possibility to change metadata from universe or even add a new key in the metadata. The CV_service takes into account this specifity ! so for the activity, we can use the terms in universe as it. change the context : just change the end of the \"base\" into the data descriptor name : \"@base\": \"http://es-vocab.ipsl.fr/Activity\" } in the graph, add the list of id from universe that we need for CMIP6Plus. For now there are only 2 ( CMIP, LESFMIP). We will do it by hand. { \"@context\":{ \"@base\": \"http://es-vocab.ipsl.fr/Activity\" }, \"@graph\":[ {\"@id\":\"cmip\", \"@type\":\"activity\" }, { \"@id\":\"lesmip\", \"@type\":\"activity\"} ] } push change git add . git commmit -m \"add activity collection\" git push --set upstream origin activity and create pull request in github interface that will be done asap. let Magic happen the github CI/CD will inform the CV service to restart. Test it to test the cv service is up to date, we will use a simple curl to the API. documentation about the api is here : http://es-vocab.ipsl.fr/docs curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/project/CMIP6Plus_CVs/collection/activity/term' \\ -H 'accept: application/json' 2 problems : * activity in CMIP6 in fact activity_id . Just change the name of the collection file. It will still select id from the activity data descriptor in WGCM_CVs there is not lesmip, it is a type during the procedure. it is lesfmip. Change the id inside the collection curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/project/CMIP6Plus_CVs/collection/activity_id/term' \\ -H 'accept: application/json' result : [\"cmip\",\"lesfmip\"] Perfect !","title":"Datadescriptor creation"},{"location":"DD_creation_example/#adding-a-new-datadescriptor","text":"Lets take an example to illustrate the procedure : we will add the activity datadescriptor","title":"Adding a new datadescriptor"},{"location":"DD_creation_example/#1-define-the-linkml-model","text":"the linkml model permits to automate differents things that are usefull to deal with the controled vocabulary: generate jsonschema model, generate pydantic model. Depends on what you prefer generate jsonld validation of terms","title":"1. Define the Linkml model"},{"location":"DD_creation_example/#get-actual-universe-repository","text":"git clone https://github.com/ESPRI-Mod/WGCM_CVs.git","title":"get actual Universe repository"},{"location":"DD_creation_example/#swith-to-a-new-branch","text":"git checkout -b newDD","title":"swith to a new branch"},{"location":"DD_creation_example/#1-define-the-linkml-model_1","text":"the linkml models are in the directory \"src/wgcm_cvs/schemas\". we can look at one similar datadescriptor in order to create the new one. There 3 distincts methods to define the CVs : list of valid terms a regex to valid a term a composite that can be a composition of the previous one.","title":"1. Define the linkml model"},{"location":"DD_creation_example/#copy-existing-similar-data-descriptor","text":"in our example activity . This a list of valid term. institution.yaml is also a list of term. this will be our starting point. Lets copy the latter as activity.yaml .","title":"Copy existing similar data descriptor"},{"location":"DD_creation_example/#changes-according-to-our-new-data-descriptor","text":"id : change the id and the name accordindly the prefixes used by the web semantic part of the service is pretty defined. just change the institution in activity in the key and the value : activity: http://es-vocab.ipsl.fr/activity/ change the default prefix to freshly created : default_prefix: activity the model classes : First thing first let's name our class like the datadesciptor with a upper first letter. classes: Activity: lets add a description : what is an activty ? description :an 'activity' refers to a coordinated set of modeling experiments designed to address specific scientific questions or objectives. Each activity is focused on different aspects of climate science and utilizes various models to study a wide range of climate phenomena. Activities are often organized around key research themes and may involve multiple experiments, scenarios, and model configurations. class_uri is used to define the appropriate type in the context of web semantic : lets use our prefix and datadescriptor name : class_uri: esvocab:activity the main course: the attibutes. the basic one : change the slot_uri validation_method doesn't have to change since we have started from a descriptor that have the same validation method . here a list of terms here we have to define the model (i.e keys) for our datadescriptor : activity is defined with a standard acronym and a long name. We just have to keep name thats will be the same as cmip_acronym and create a long_name . In the new CMIP6Plus CV, there is also an url to document the activity. Lets add a url * attribute to our model attributes: id: slot_uri: activity:id range: string required: true identifier: true validation_method : required: true range : string ifabsent: string(list) slot_uri: es-vocab:validation_method name: range: string required: true long_name: range: string required: true cmip_acronym: range: string required: true url: range : string required: false the linkml model is defined. This will be used by the repository to validate the defined standard terms.","title":"Changes according to our new data descriptor"},{"location":"DD_creation_example/#2-define-terms","text":"As a starting point for activity terms. we will use the known activities from CMIP6 and CMIP6Plus : for CMIP6 : \"activity_id\":{ \"AerChemMIP\":\"Aerosols and Chemistry Model Intercomparison Project\", \"C4MIP\":\"Coupled Climate Carbon Cycle Model Intercomparison Project\", \"CDRMIP\":\"Carbon Dioxide Removal Model Intercomparison Project\", \"CFMIP\":\"Cloud Feedback Model Intercomparison Project\", \"CMIP\":\"CMIP DECK: 1pctCO2, abrupt4xCO2, amip, esm-piControl, esm-historical, historical, and piControl experiments\", \"CORDEX\":\"Coordinated Regional Climate Downscaling Experiment\", \"DAMIP\":\"Detection and Attribution Model Intercomparison Project\", \"DCPP\":\"Decadal Climate Prediction Project\", \"DynVarMIP\":\"Dynamics and Variability Model Intercomparison Project\", \"FAFMIP\":\"Flux-Anomaly-Forced Model Intercomparison Project\", \"GMMIP\":\"Global Monsoons Model Intercomparison Project\", \"GeoMIP\":\"Geoengineering Model Intercomparison Project\", \"HighResMIP\":\"High-Resolution Model Intercomparison Project\", \"ISMIP6\":\"Ice Sheet Model Intercomparison Project for CMIP6\", \"LS3MIP\":\"Land Surface, Snow and Soil Moisture\", \"LUMIP\":\"Land-Use Model Intercomparison Project\", \"OMIP\":\"Ocean Model Intercomparison Project\", \"PAMIP\":\"Polar Amplification Model Intercomparison Project\", \"PMIP\":\"Palaeoclimate Modelling Intercomparison Project\", \"RFMIP\":\"Radiative Forcing Model Intercomparison Project\", \"SIMIP\":\"Sea Ice Model Intercomparison Project\", \"ScenarioMIP\":\"Scenario Model Intercomparison Project\", \"VIACSAB\":\"Vulnerability, Impacts, Adaptation and Climate Services Advisory Board\", \"VolMIP\":\"Volcanic Forcings Model Intercomparison Project\" }, for CMIP6Plus : \"CMIP\": { \"URL\": \"https://gmd.copernicus.org/articles/9/1937/2016/gmd-9-1937-2016.pdf\", \"long_name\": \"CMIP DECK: 1pctCO2, abrupt4xCO2, amip, esm-piControl, esm-historical, historical, and piControl experiments\" }, \"LESFMIP\": { \"URL\": \"https://www.frontiersin.org/articles/10.3389/fclim.2022.955414/full\", \"long_name\": \"The Large Ensemble Single Forcing Model Intercomparison Project\" } in the directory \"data_decriptor\" : create a activity/terms directories. Inside one term is define in one json file. We will use a python script to create those. a little script to do that is stored in \"script/DD_specific/create_activity.py\". Eventually terms a created : an example : pmip.json { \"id\": \"pmip\", \"cmip_acronym\": \"PMIP\", \"long_name\": \"Palaeoclimate Modelling Intercomparison Project\", \"url\": null }","title":"2. Define terms"},{"location":"DD_creation_example/#3-validate-model-and-terms-locally","text":"it is possible to validate model and terms locally tahnks to linkml library : for instance : pdm run linkml-validate --schema schemas/activity.yaml --target-class ../../data_descriptors/source/terms/pmip.json Or you can let github action do this :","title":"3. Validate model and terms locally"},{"location":"DD_creation_example/#4-here-magic-happen","text":"","title":"4. Here Magic happen"},{"location":"DD_creation_example/#push-changes","text":"git commit -am \"create activity datadescriptor and terms\" git push --set-upstream origin activityDD","title":"Push changes"},{"location":"DD_creation_example/#validation","text":"then in github interface on WGCM_CVs repository","title":"validation"},{"location":"DD_creation_example/#cicd-validation","text":"You have to wait for CI/CD to check that everything is good, if not the merge will be cancel ! what does the CI/CD : create pydantic and jsonscheme model in \"datadescriptors/activity/models\" create the jsonld file version of each terms in \"datadesciptors/activity/terms\" validate the terms with the models. validation CI/CD can be view in Action tab.","title":"CI/CD : Validation"},{"location":"DD_creation_example/#for-instance-with-this-procedure","text":"the validate failed because there was a type in the linkml file name ! the validate failed cause of pydantic model why ? here the problem need to be tested in local. we can run the model creation and validate script : pdm run python scripts/generate_models.py the models are stored next to terms, i cant see problem with the model. lets try to validate locally pdm run python scripts/validate.py it validates everythings through json schema but not pydantic : Field required [type=missing, input_value={'id': 'aerchemmip', 'cmi...n Project', 'url': None}, input_type=dict] there is a missing field, after investigation : the python script that create the terms does not include name attribute. Lets repair that ! pdm run python scripts/validate.py YES !, validation is OK ! lets push changes to the origin repository","title":"for instance with this procedure :"},{"location":"DD_creation_example/#create-a-pull-request","text":"if validation step went well, create a pull request. This will be merged into main asap.","title":"Create a pull request"},{"location":"DD_creation_example/#the-magic","text":"github repository is configured to send a message to the CV Service. It will be update automaticaly !!!","title":"The magic"},{"location":"DD_creation_example/#define-projects-collection","text":"Each project does not necesseraly use or even valid all the terms in the univers data descriptor. collections of valid id have to be define in each project : for instance, we will add activity collection id for CMIP6Plus_CVs :","title":"Define projects collection"},{"location":"DD_creation_example/#clone-the-repo","text":"git clone https://github.com/ESPRI-Mod/CMIP6Plus_CVs.git","title":"clone the repo"},{"location":"DD_creation_example/#create-a-new-branch","text":"git checkout -b activity","title":"create a new branch"},{"location":"DD_creation_example/#create-collection","text":"in directory collections : Lets start from the institution.json and copy it as activity.json { \"@context\":{ \"@base\": \"http://es-vocab.ipsl.fr/Institution\" }, \"@graph\":[ {\"@id\":\"ipsl\", \"@type\":\"institution\", \"name\" : \"Institut Pierre-Simon Laplace modified\", \"location\":{ \"city\":\"Paris5\" }, \"myprop\":\"42\" }, { \"@id\":\"llnl\", \"@type\":\"institution\"} ] } this one is a show case to view the possibility to change metadata from universe or even add a new key in the metadata. The CV_service takes into account this specifity ! so for the activity, we can use the terms in universe as it. change the context : just change the end of the \"base\" into the data descriptor name : \"@base\": \"http://es-vocab.ipsl.fr/Activity\" } in the graph, add the list of id from universe that we need for CMIP6Plus. For now there are only 2 ( CMIP, LESFMIP). We will do it by hand. { \"@context\":{ \"@base\": \"http://es-vocab.ipsl.fr/Activity\" }, \"@graph\":[ {\"@id\":\"cmip\", \"@type\":\"activity\" }, { \"@id\":\"lesmip\", \"@type\":\"activity\"} ] }","title":"create collection"},{"location":"DD_creation_example/#push-change","text":"git add . git commmit -m \"add activity collection\" git push --set upstream origin activity and create pull request in github interface that will be done asap.","title":"push change"},{"location":"DD_creation_example/#let-magic-happen","text":"the github CI/CD will inform the CV service to restart.","title":"let Magic happen"},{"location":"DD_creation_example/#test-it","text":"to test the cv service is up to date, we will use a simple curl to the API. documentation about the api is here : http://es-vocab.ipsl.fr/docs curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/project/CMIP6Plus_CVs/collection/activity/term' \\ -H 'accept: application/json' 2 problems : * activity in CMIP6 in fact activity_id . Just change the name of the collection file. It will still select id from the activity data descriptor in WGCM_CVs there is not lesmip, it is a type during the procedure. it is lesfmip. Change the id inside the collection curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/project/CMIP6Plus_CVs/collection/activity_id/term' \\ -H 'accept: application/json' result : [\"cmip\",\"lesfmip\"] Perfect !","title":"Test it"},{"location":"restapi/","text":"REST API The REST API is a core component of the es-vocab service, providing programmatic access to the controlled vocabularies. It allows users to retrieve term definitions and associated metadata efficiently. What is the REST API? The REST API of es-vocab is an interface designed for programmatic access to the controlled vocabularies. It leverages the FastAPI framework to deliver a robust and high-performance API with automatically generated documentation. Why use the REST API? Using the REST API provides several benefits: Automation : Enables automated systems and applications to access vocabulary terms and metadata without manual intervention. Efficiency : Provides quick and consistent access to standardized terms, improving data processing and integration workflows. Interoperability : Facilitates seamless communication between different systems and services by using a common vocabulary. Scalability : Supports large-scale data access and manipulation, essential for big data applications and services. How to use the REST API? The REST API is hosted at http://es-vocab.ipsl.fr . Below are some common usage examples and endpoints. Endpoints Swagger api documentation http://es-vocab.ipsl.fr/docs Retrieve institution Term Metadata curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/universe/datadescriptor/institution/term' \\ -H 'accept: application/json'","title":"API"},{"location":"restapi/#rest-api","text":"The REST API is a core component of the es-vocab service, providing programmatic access to the controlled vocabularies. It allows users to retrieve term definitions and associated metadata efficiently.","title":"REST API"},{"location":"restapi/#what-is-the-rest-api","text":"The REST API of es-vocab is an interface designed for programmatic access to the controlled vocabularies. It leverages the FastAPI framework to deliver a robust and high-performance API with automatically generated documentation.","title":"What is the REST API?"},{"location":"restapi/#why-use-the-rest-api","text":"Using the REST API provides several benefits: Automation : Enables automated systems and applications to access vocabulary terms and metadata without manual intervention. Efficiency : Provides quick and consistent access to standardized terms, improving data processing and integration workflows. Interoperability : Facilitates seamless communication between different systems and services by using a common vocabulary. Scalability : Supports large-scale data access and manipulation, essential for big data applications and services.","title":"Why use the REST API?"},{"location":"restapi/#how-to-use-the-rest-api","text":"The REST API is hosted at http://es-vocab.ipsl.fr . Below are some common usage examples and endpoints.","title":"How to use the REST API?"},{"location":"restapi/#endpoints","text":"","title":"Endpoints"},{"location":"restapi/#swagger-api-documentation","text":"http://es-vocab.ipsl.fr/docs","title":"Swagger api documentation"},{"location":"restapi/#retrieve-institution-term-metadata","text":"curl -X 'GET' \\ 'http://es-vocab.ipsl.fr/api/universe/datadescriptor/institution/term' \\ -H 'accept: application/json'","title":"Retrieve institution Term Metadata"},{"location":"sparqlendpoint/","text":"SPARQL Endpoint What The SPARQL endpoint offers a powerful query interface to access and manipulate vocabulary data using the SPARQL query language, specifically designed for querying RDF (Resource Description Framework) data. Why The SPARQL endpoint enhances websemantic interoperability by enabling users to perform sophisticated and detailed queries on the vocabulary data. It also facilitates the seamless integration with other semantic web technologies and datasets, fostering a more interconnected and efficient data environment. How To utilize the SPARQL endpoint, simply send your SPARQL queries to the provided endpoint URL. These queries can be used to retrieve, filter, and manipulate the vocabulary data according to your needs. YASGUI interface you can find a yasgui interface : https://es-vocab.ipsl.fr/sparql Basic query To get all triples in graph : SELECT * WHERE {?s ?p ?o .} Institution query SELECT * WHERE {?s <http://es-vocab.ipsl.fr/institution/acronyms> ?o .}","title":"SPARQL"},{"location":"sparqlendpoint/#sparql-endpoint","text":"","title":"SPARQL Endpoint"},{"location":"sparqlendpoint/#what","text":"The SPARQL endpoint offers a powerful query interface to access and manipulate vocabulary data using the SPARQL query language, specifically designed for querying RDF (Resource Description Framework) data.","title":"What"},{"location":"sparqlendpoint/#why","text":"The SPARQL endpoint enhances websemantic interoperability by enabling users to perform sophisticated and detailed queries on the vocabulary data. It also facilitates the seamless integration with other semantic web technologies and datasets, fostering a more interconnected and efficient data environment.","title":"Why"},{"location":"sparqlendpoint/#how","text":"To utilize the SPARQL endpoint, simply send your SPARQL queries to the provided endpoint URL. These queries can be used to retrieve, filter, and manipulate the vocabulary data according to your needs.","title":"How"},{"location":"sparqlendpoint/#yasgui-interface","text":"you can find a yasgui interface : https://es-vocab.ipsl.fr/sparql","title":"YASGUI interface"},{"location":"sparqlendpoint/#basic-query","text":"To get all triples in graph : SELECT * WHERE {?s ?p ?o .}","title":"Basic query"},{"location":"sparqlendpoint/#institution-query","text":"SELECT * WHERE {?s <http://es-vocab.ipsl.fr/institution/acronyms> ?o .}","title":"Institution query"},{"location":"website/","text":"Website (WIP) The website is designed for human users to browse and search for terms and their definitions. It provides an intuitive interface for navigating the controlled vocabularies. The web adress of each term is also term IRI used in sparql endpoint. Features Search : Quickly find terms by their names or descriptions. Browse : Explore terms by category or project. Details : View detailed metadata for each term.","title":"WebSite"},{"location":"website/#website-wip","text":"The website is designed for human users to browse and search for terms and their definitions. It provides an intuitive interface for navigating the controlled vocabularies. The web adress of each term is also term IRI used in sparql endpoint.","title":"Website (WIP)"},{"location":"website/#features","text":"Search : Quickly find terms by their names or descriptions. Browse : Explore terms by category or project. Details : View detailed metadata for each term.","title":"Features"}]}